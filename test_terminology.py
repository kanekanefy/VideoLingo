#!/usr/bin/env python3
"""
ä¸“ä¸šåè¯ç®¡ç†ç³»ç»Ÿæµ‹è¯•è„šæœ¬

æµ‹è¯•æ ¸å¿ƒåŠŸèƒ½ï¼Œä¸ä¾èµ–spacyã€pandasç­‰é‡å‹åº“
"""

import json
import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°ç³»ç»Ÿè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

def test_basic_term_management():
    """æµ‹è¯•åŸºç¡€æœ¯è¯­ç®¡ç†åŠŸèƒ½"""
    print("ğŸ§ª å¼€å§‹æµ‹è¯•æœ¯è¯­ç®¡ç†åŸºç¡€åŠŸèƒ½...")
    
    try:
        # åˆ›å»ºæµ‹è¯•ç›®å½•
        test_output_dir = Path("test_output/terminology")
        test_output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºç®€åŒ–çš„æœ¯è¯­ç®¡ç†å™¨
        class SimpleTermManager:
            def __init__(self, project_dir="test_output"):
                self.project_dir = Path(project_dir)
                self.terminology_dir = self.project_dir / "terminology"
                self.terminology_dir.mkdir(parents=True, exist_ok=True)
                self.custom_terms_file = self.terminology_dir / "custom_terms.json"
                self.custom_terms = self._load_custom_terms()
            
            def _load_custom_terms(self):
                if self.custom_terms_file.exists():
                    with open(self.custom_terms_file, 'r', encoding='utf-8') as f:
                        return json.load(f)
                return {
                    "version": "1.0",
                    "terms": {},
                    "categories": {},
                    "priorities": {},
                    "notes": {}
                }
            
            def add_term(self, source, target, category="general", priority=1, notes=""):
                self.custom_terms["terms"][source] = target
                self.custom_terms["categories"][source] = category
                self.custom_terms["priorities"][source] = priority
                self.custom_terms["notes"][source] = notes
                self._save_terms()
                return True
            
            def _save_terms(self):
                with open(self.custom_terms_file, 'w', encoding='utf-8') as f:
                    json.dump(self.custom_terms, f, ensure_ascii=False, indent=2)
            
            def get_all_terms(self):
                return self.custom_terms["terms"]
            
            def apply_terms_to_text(self, text):
                applied_terms = []
                result_text = text
                for source, target in self.custom_terms["terms"].items():
                    if source in result_text:
                        result_text = result_text.replace(source, target)
                        applied_terms.append({"source": source, "target": target})
                return result_text, applied_terms
        
        # åˆ›å»ºæµ‹è¯•å®ä¾‹
        term_manager = SimpleTermManager()
        print("âœ… æœ¯è¯­ç®¡ç†å™¨åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•æ·»åŠ æœ¯è¯­
        test_terms = [
            ("AI", "äººå·¥æ™ºèƒ½", "æŠ€æœ¯æœ¯è¯­", 5, "æ ¸å¿ƒæŠ€æœ¯æ¦‚å¿µ"),
            ("machine learning", "æœºå™¨å­¦ä¹ ", "æŠ€æœ¯æœ¯è¯­", 4, "AIå­é¢†åŸŸ"),
            ("OpenAI", "OpenAI", "å“ç‰Œ", 3, "AIå…¬å¸"),
            ("neural network", "ç¥ç»ç½‘ç»œ", "æŠ€æœ¯æœ¯è¯­", 4, "æ·±åº¦å­¦ä¹ åŸºç¡€"),
            ("ChatGPT", "ChatGPT", "äº§å“", 3, "å¯¹è¯AIäº§å“")
        ]
        
        for source, target, category, priority, notes in test_terms:
            success = term_manager.add_term(source, target, category, priority, notes)
            if success:
                print(f"âœ… æˆåŠŸæ·»åŠ æœ¯è¯­: {source} â†’ {target}")
            else:
                print(f"âŒ æ·»åŠ æœ¯è¯­å¤±è´¥: {source}")
        
        # æµ‹è¯•æœ¯è¯­åº”ç”¨
        test_text = "AI and machine learning are core technologies. OpenAI developed ChatGPT using neural network."
        corrected_text, applied_terms = term_manager.apply_terms_to_text(test_text)
        
        print(f"\nğŸ”„ æœ¯è¯­åº”ç”¨æµ‹è¯•:")
        print(f"åŸæ–‡: {test_text}")
        print(f"ä¿®æ­£å: {corrected_text}")
        print(f"åº”ç”¨çš„æœ¯è¯­æ•°é‡: {len(applied_terms)}")
        
        for term in applied_terms:
            print(f"  - {term['source']} â†’ {term['target']}")
        
        # æµ‹è¯•æœ¯è¯­åº“ç»Ÿè®¡
        all_terms = term_manager.get_all_terms()
        print(f"\nğŸ“Š æœ¯è¯­åº“ç»Ÿè®¡:")
        print(f"æ€»æœ¯è¯­æ•°: {len(all_terms)}")
        
        # æŒ‰åˆ†ç±»ç»Ÿè®¡
        categories = {}
        for source in all_terms:
            category = term_manager.custom_terms["categories"].get(source, "æœªåˆ†ç±»")
            categories[category] = categories.get(category, 0) + 1
        
        for category, count in categories.items():
            print(f"  - {category}: {count}ä¸ª")
        
        print("\nâœ… åŸºç¡€åŠŸèƒ½æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_term_extraction():
    """æµ‹è¯•æœ¯è¯­æå–åŠŸèƒ½ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    print("\nğŸ” æµ‹è¯•æœ¯è¯­æå–åŠŸèƒ½...")
    
    try:
        import re
        
        def simple_extract_terms(text):
            """ç®€åŒ–çš„æœ¯è¯­æå–"""
            # æå–å¤§å†™è¯æ±‡
            uppercase_terms = re.findall(r'\b[A-Z]{2,}\b', text)
            
            # æå–é¦–å­—æ¯å¤§å†™çš„ä¸“ä¸šè¯æ±‡
            capitalized_terms = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', text)
            
            # æå–è¿å­—ç¬¦è¯æ±‡
            hyphenated_terms = re.findall(r'\b[a-zA-Z]+-[a-zA-Z]+\b', text)
            
            return {
                "uppercase": uppercase_terms,
                "capitalized": capitalized_terms,
                "hyphenated": hyphenated_terms
            }
        
        # æµ‹è¯•æ–‡æœ¬
        test_texts = [
            "The AI system uses machine learning algorithms like CNN and RNN.",
            "OpenAI developed GPT-4 using transformer architecture.",
            "Deep learning is a subset of machine learning in artificial intelligence."
        ]
        
        for i, text in enumerate(test_texts):
            print(f"\næ–‡æœ¬ {i+1}: {text}")
            extracted = simple_extract_terms(text)
            
            for term_type, terms in extracted.items():
                if terms:
                    print(f"  {term_type}: {', '.join(set(terms))}")
        
        print("\nâœ… æœ¯è¯­æå–æµ‹è¯•å®Œæˆï¼")
        return True
        
    except Exception as e:
        print(f"âŒ æœ¯è¯­æå–æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_file_operations():
    """æµ‹è¯•æ–‡ä»¶æ“ä½œåŠŸèƒ½"""
    print("\nğŸ“ æµ‹è¯•æ–‡ä»¶æ“ä½œåŠŸèƒ½...")
    
    try:
        # æµ‹è¯•æœ¯è¯­åº“æ–‡ä»¶è¯»å†™
        test_file = "test_output/terminology/test_terms.json"
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_data = {
            "version": "1.0",
            "terms": {
                "API": "åº”ç”¨ç¨‹åºæ¥å£",
                "SDK": "è½¯ä»¶å¼€å‘å·¥å…·åŒ…",
                "REST": "è¡¨è¿°æ€§çŠ¶æ€ä¼ é€’"
            },
            "categories": {
                "API": "æŠ€æœ¯æœ¯è¯­",
                "SDK": "æŠ€æœ¯æœ¯è¯­", 
                "REST": "æŠ€æœ¯æœ¯è¯­"
            }
        }
        
        # å†™å…¥æ–‡ä»¶
        with open(test_file, 'w', encoding='utf-8') as f:
            json.dump(test_data, f, ensure_ascii=False, indent=2)
        print(f"âœ… æˆåŠŸå†™å…¥æµ‹è¯•æ–‡ä»¶: {test_file}")
        
        # è¯»å–æ–‡ä»¶
        with open(test_file, 'r', encoding='utf-8') as f:
            loaded_data = json.load(f)
        
        # éªŒè¯æ•°æ®
        if loaded_data["terms"] == test_data["terms"]:
            print("âœ… æ–‡ä»¶è¯»å†™éªŒè¯æˆåŠŸ")
        else:
            print("âŒ æ–‡ä»¶è¯»å†™éªŒè¯å¤±è´¥")
            return False
        
        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        os.remove(test_file)
        print("âœ… æµ‹è¯•æ–‡ä»¶æ¸…ç†å®Œæˆ")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ–‡ä»¶æ“ä½œæµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ VideoLingo ä¸“ä¸šåè¯ç®¡ç†ç³»ç»Ÿæµ‹è¯•")
    print("=" * 50)
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    tests = [
        ("åŸºç¡€æœ¯è¯­ç®¡ç†", test_basic_term_management),
        ("æœ¯è¯­æå–åŠŸèƒ½", test_term_extraction),
        ("æ–‡ä»¶æ“ä½œåŠŸèƒ½", test_file_operations)
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        print(f"\nğŸ§ª è¿è¡Œæµ‹è¯•: {test_name}")
        print("-" * 30)
        
        if test_func():
            passed += 1
            print(f"âœ… {test_name} æµ‹è¯•é€šè¿‡")
        else:
            print(f"âŒ {test_name} æµ‹è¯•å¤±è´¥")
    
    print("\n" + "=" * 50)
    print(f"ğŸ“Š æµ‹è¯•ç»“æœ: {passed}/{total} ä¸ªæµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡äº†ï¼ä¸“ä¸šåè¯ç®¡ç†ç³»ç»ŸåŸºç¡€åŠŸèƒ½æ­£å¸¸ã€‚")
        print("\nğŸ“ ä¸‹ä¸€æ­¥æµ‹è¯•å»ºè®®:")
        print("1. å®‰è£…å®Œæ•´ç¯å¢ƒåæµ‹è¯•Streamlitç•Œé¢")
        print("2. æµ‹è¯•ä¸VideoLingoç¿»è¯‘æµç¨‹çš„é›†æˆ")
        print("3. æµ‹è¯•Excelå¯¼å…¥å¯¼å‡ºåŠŸèƒ½")
        return True
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯å¹¶ä¿®å¤ã€‚")
        return False

if __name__ == "__main__":
    main()